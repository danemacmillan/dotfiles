# vim: ft=sh

##
# Check if command line utility exists.
#
# This is based on discussion at:
# http://stackoverflow.com/q/592620
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
#
command_exists()
{
	command -v "$1" >/dev/null 2>&1
}

##
# Return a usable timestamp for filenames.
#
# Example usage:
#  local file_name="backup.$(now).sql.gz"
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
now()
{
	echo $(date +'%Y%m%d%H%M%S-%b-%d-%a-%H%M')
}

##
# Calculate MD5 hashes regardless of tool, and ensure identical output.
#
# The file contents themselves are hashed when a valid file path is provided,
# but when it is determined that the path provided is not a valid file path,
# it will be treated as a string and that string is hashed. The latter operation
# is done by piping to STDIN via the "<<<" operator.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
calculate_md5_hash()
{
	local command_name="calculate_md5_hash"
	local md5tool=md5sum
	local cut_offset=1
	local file_path="${1}"

	if [[ -z "${file_path}" ]]; then
		echo -e "Calculate the MD5 hash of a file (or string if path not found)"
		echo -e "Usage:\n  ${BCYAN}${command_name}${RESET} ${GREEN}FILEPATH|STRING"
		return 1
	fi

	# OSX comes with md5 tool by default, not md5sum.
	if hash md5 2>/dev/null; then
		local md5tool=md5
		local cut_offset=4
	fi

	if [[ -f "${file_path}" ]]; then
		echo $($md5tool "${file_path}" | cut -d " " -f$cut_offset)
	else
		echo $($md5tool <<< "${file_path}" | cut -d " " -f$cut_offset)
	fi
}

##
# Remotely run commands with color TTY. If passing chained commands (&&),
# remember to put them within quotation marks.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
rr()
{
	if (( $# > 1 )); then
		local remote_server="$1"
		local remote_commands="${@:2}"
		echo -e "${BLUE}${BOLD}Running '\x1B[97;1m$remote_commands${BLUE}' on '$remote_server':${RESET}"
		ssh -qtt $remote_server "$remote_commands"
	fi
}

##
# Loop through all subdirectories and run the given command."
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
loop()
{
	if [[ -z $1 ]]; then
		echo -e "Loop through all subdirectories of the current directory and run"
		echo -e "the given commands.\n"
		echo -e "Example usage:\n"
		echo '  loop "touch notes.txt"'
		echo '  loop "git pull" "git push"'
		echo ""
		return
	fi

	for subdirectory in ./*; do
		echo -e "\n${RESET}\x1B[48;5;235m${TWIRL} \x1B[97;1mRunning commands in ${BBLUE}$subdirectory${RESET}"
		for argument in "$@"; do
			echo -e "   ${BOLT} \x1B[93m$argument${RESET}\x1B[38;5;245m"
			(cd "$subdirectory" && $argument 2>&1 | sed -e 's/^/        /')
		done
	done

	echo -e ""
}

##
# Create a local SSH tunnel to a remote service.
# This is just a stub to remember the command.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
tun()
{
	local username="$1"
	local service_local="$2"
	local service_remote="$3"
	ssh -f -T -N -L $service_local:$service_remote -pPORT $username@$remote_server
}

##
# Remotely execute SQL and save output locally.
# This is just a stub to remember the command.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
rrmy()
{
	ssh -T -pPORT USERNAME@SERVER "mysql -u USERNAME -pPASSWORD -Bs" < "cool.sql" > output.txt
	# Or if no script to execute, use -Bse.
}

##
# Merge all canonical branches from current directory.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
choochoo()
{
	echo -e "${BLUE}${BOLD}Running a train on this repo. Hiyoooo!${RESET}"
	git checkout develop && git pull && git checkout stage && git pull && git merge develop && git push && git checkout master && git pull && git merge stage && git push && git checkout develop && echo -e "All canonical branches merged up to master."
}

##
# Get current OS version and other information.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
v()
{
	case $OS in
		osx)
			system_profiler SPSoftwareDataType
			;;
		# Note, at this point nix is technically CentOS. Update .bashrc to
		# represent more OSes.
		nix)
			echo -e "Operating System:\n"
			echo -e "    `cat /etc/redhat-release`\n"
			;;
	esac

	echo -e "Kernel & Architecture:\n"
	echo -e "    `uname -r -v -p`\n"
}

##
# Get total directory and file count.
# TODO: integrate filesizes
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
tt()
{
	CHECKDIR=.
	if [[ -n "$1" ]]; then
		CHECKDIR="$1"
	fi

	# Directory listings include "." and ".."
	DIRECTORY_PADDING=1

	DIRECTORIES="`find $CHECKDIR -maxdepth 1 -type d | wc -l`"
	DIRECTORIES_WITH_LINKS="`find $CHECKDIR -follow -maxdepth 1 -type d | wc -l`"
	DIRECTORIES_LINKS="`expr $DIRECTORIES_WITH_LINKS - $DIRECTORIES`"
	FILES="`find $CHECKDIR -maxdepth 1 -type f | wc -l`"
	FILES_WITH_LINKS="`find $CHECKDIR -follow -maxdepth 1 -type f | wc -l`"
	FILES_LINKS="`expr $FILES_WITH_LINKS - $FILES`"

	echo "Directories: `expr $DIRECTORIES_WITH_LINKS - $DIRECTORY_PADDING` (`expr $DIRECTORIES_LINKS`)"
	echo "Files: $FILES_WITH_LINKS ($FILES_LINKS)"
}

##
# Auto-generate new SSH keys with option to provide passphrase
# Note: if you want characters like "!" in your passphrase, run like this:
# newsshkey 'Long passphrase!!!! with spaces and niceties!'
# If you just have regular characters, quoting is unnecessary.
#
# Notes
# SSH keys, a la http://blog.patshead.com/2013/09/generating-new-more-secure-ssh-keys.html
# ssh-keygen -b 4096 -f ~/.ssh/id_rsa_danemacmillan_4096_2014_08 -C danemacmillan@id_rsa_4096_2014_08
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
newsshkey()
{
	KEY_USER=${USER}
	if [[ -n $1 ]]; then
		KEY_USER="$1"
	fi

	KEY_STRENGTH="4096"
	KEY_NAME_BASE=${KEY_STRENGTH}_$(date +%Y_%m_%d_%H%M%S)
	KEY_PATH=${HOME}/.ssh/${KEY_USER}_${KEY_NAME_BASE}.id_rsa
	KEY_COMMENT="${KEY_USER}@${KEY_NAME_BASE}"

	KEY_PASSPHRASE=""
	if [[ -n "$2" ]]; then
		KEY_PASSPHRASE="$2"
	fi

	ssh-keygen -b ${KEY_STRENGTH} -f ${KEY_PATH} -C ${KEY_COMMENT} -N "${KEY_PASSPHRASE}"

	echo -e "\nUsername:" \"${KEY_USER}\"
	echo -e "\nPassphrase:" \"${KEY_PASSPHRASE}\"
	echo -e "\nProvide this public key to your devop if required:\n"
	cat ${KEY_PATH}.pub

	# Remove passphrase from appearing in bash history, if provided.
	# Source: http://thoughtsbyclayg.blogspot.ca/2008/02/how-to-delete-last-command-from-bash.html
	if [[ -n "$1" ]]; then
		history -d $((HISTCMD-2)) && history -d $((HISTCMD-1))
	fi
}

##
# Find all Magento events in current path or path provided.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
magevents()
{
	if hash ag 2>/dev/null ; then
		local magento_path=""
		if [[ -n "$1" ]]; then
			magento_path="$1"
			ag --php -A 5 -C 0 --depth 50 -f -i "Mage::dispatchEvent" "$magento_path"
		else
			echo "Provide path to search for Magento events."
		fi
	else
		echo "ag / silver_searcher is required."
	fi
}

##
# Pass a filename, with archive extension, and this will figure out the rest.
#
# Usage: archive <desiredfilename>.<extension> [ directory | filename ]
#
# For example, type "archive updates.tar.gz updatesdir" and this function will run the
# necessary commands for generating an archive with .tar.gz compression.
#
# Note: bz2 exclusively requires that you just pass bz2 as the desiredfilename
# because it cannot be output into a single file, but each file will be
# individually archived with bzip2 compression.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
#
# @TODO This is incomplete and requires more testing.
archive()
{
	case $1 in
		*bz2)       bzip2 "${@:2}" ;;
		*.gz)       gzip -c "${@:2}" > $1 ;;
		*.tar)      tar -cvf $1 "${@:2}" ;;
		*.tbz)      tar -jcvf $1 "${@:2}" ;;
		*.tbz2)     tar -jcvf $1 "${@:2}" ;;
		*.tar.bz2)  tar -jcvf $1 "${@:2}" ;;
		*.tgz)      tar -zcvf $1 "${@:2}" ;;
		*.tar.gz)   tar -zcvf $1 "${@:2}" ;;
		*.zip)      zip -r $1 "${@:2}" ;;
		*.ZIP)      zip -r $1 "${@:2}" ;;
		#*.pax)      cat $1 | pax -r ;;
		#*.pax.Z)    uncompress $1 --stdout | pax -r ;;
		#*.Z)        uncompress $1 ;;
		#*.dmg)      hdiutil mount $1 ;;
		*)          echo "'${@:2}' cannot be archived via archive()" ;;
	esac
}

##
# Get IP count of standard Web server logs (Nginx/Apache)
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
log_ips()
{
	if [[ -n $1 ]]; then
		cat $1 | awk '{print $1}' | sort -n | uniq -c | sort -nr | head -30
	fi
}

##
# Check if given user agent has access to the given URL.
#
# Example usage:
#  curl -I -H 'User-agent: Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0' https://www.canadasmotorcycle.ca
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
hasaccess()
{
	curl -I -H 'User-agent: "$1"' "$2"
}

##
# Find Firefox install on OSX, so commands can be passed to it.
#
# TODO: Perform search on different platforms. Check for existence before.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
firefox()
{
	# OSX only
	$(mdfind -onlyin /Applications/ -name firefox)/Contents/MacOS/firefox "$@"
}

##
# Load specified Firefox profile.
#
# Pass a specific profile name and it will load, whether an instance of Firefox
# is already open or not.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
firefox_profile()
{
	firefox --no-remote -P "$@"
}

##
# A better cat utility.
#
# This is for lazy people who just want to cat out documents to their terminal.
# These documents can be any kind of text document, as usual, but also any
# kind of image, assuming the `imgcat` script is available, as well as iterm2.
# Additionally, this will handle remote files over HTTP and HTTPS.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
cat()
{
	local filepath="${1}"

	case "${filepath}" in
		http:*|https:*)
			local md5hashpathstring=$(calculate_md5_hash "${filepath}")
			filepath=$(wget "${filepath}" -q -O "${HOME}/tmp/${md5hashpathstring}.tmp" && echo "${HOME}/tmp/${md5hashpathstring}.tmp")
			echo -e "Remote file temporarily stored at: ${filepath}"
		;;
	esac

	if command_exists file \
		&& command_exists imgcat \
	; then
		local filetype=$(file -b "${filepath}")

		case "${filetype}" in
			*JPEG*|*PNG*|*GIF*)
				imgcat "${filepath}"
				return 0
			;;
		esac
	fi

	if [[ -n "${filepath}" ]]; then
		$(which cat) "${filepath}"
		return 0
	else
		$(which cat) --help
	fi

	return 1
}

##
# Backup user directory files to GDrive using rclone.
#
# Backup very specific things on Dane MacMillan's machines.
#
# I do not recommend others run this.
#
# @TODO Modify this now that it is a function. This used to be an alias, hence
# the chaining.
#
# Also note that there are implicit EXCLUDES for rclone defined in .bashrc.
# For example, files with the ".icloud" extension are excluded.
backup()
{
  date \
  && echo 'Syncing to iCloud Drive...' \
  && rclone copy ~/ ~/Documents/dotfiles/ --include={/.ssh/**,/.weechat/irc.conf,/.boto,/.extra,/.gitconfig.local,/.rclone.conf,/linus.yaml} --copy-links \
  && rclone sync ~/hack/vagrantshell/sites/phoenix.vagrant.test/.idea/ ~/Documents/hack/phpstorm/phoenix.vagrant.test/.idea/ --transfers 10 --bwlimit off --checksum \
  && rclone move ~/Library/Group\ Containers/2BUA8C4S2C.com.agilebits/Library/Application\ Support/1Password/Backups/ ~/Documents/archives/1password/ --transfers 10 --bwlimit off \
  && rclone sync ~/Library/Preferences/ ~/Documents/preferences/ --transfers 10 --bwlimit off \
  && rclone sync ~/Pictures/Photo\ Booth\ Library/ ~/Documents/images/photobooth/ --transfers 10 --bwlimit off --checksum \
  && echo 'Backing up to Google Drive...' \
  && rclone sync ~/Desktop/ macmillanator:/Desktop/ --transfers 5 --max-size 1000M \
  && rclone copy ~/Documents/ macmillanator:/Documents/ --transfers 5 --max-size 50000M \
  && rclone sync ~/Documents/downloads/ macmillanator:/Documents/downloads/ --transfers 5 \
  && rclone sync ~/Pictures/ macmillanator:/Pictures/ --transfers 5 --checkers 20 --checksum \
  && date
}

##
# Basic function to back up Plex Media Server data.
#
# This excludes the Cache directory, as recommended in:
# https://support.plex.tv/hc/en-us/articles/201539237-Backing-Up-Plex-Media-Server-Data
#
# TODO combine with other quick backup scripts and abstract.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
backup_plexmetadata()
{
	local command_name="${FUNCNAME[0]}"

	local destination_rootpath="${HOME}"
	local destination_filename="${command_name}.$(date +\%Y\%m\%d\%H\%M\%S-\%b-\%d-\%a-\%H\%M).zip"
	local destination_path="${destination_rootpath}/${destination_filename}"
	local source_path="/var/lib/plexmediaserver"

	echo -e "${command_name} start: $(date)"

	date > "/var/log/${command_name}.log" \
		&& zip -db -vr "${destination_path}" "${source_path}" -x ${source_path}/Library/Application\ Support/Plex\ Media\ Server/Cache/\* -x ${source_path}/Library/Application\ Support/Plex\ Media\ Server/Preferences.xml -x ${source_path}/Library/Application\ Support/Plex\ Media\ Server/plexmediaserver.pid \
		&& date >> "/var/log/${command_name}.log"

	echo -e "${command_name} end zip: $(date)"

		# Very specific to me. Temporary.
	if command_exists rclone; then
		rclone -v moveto ${destination_path} gd:servers/plexbox/${destination_filename} --stats 1s \
			&& echo -e "${command_name} end transfer: $(date)"
	fi
}

##
# Back up plexdrive files and databases.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
backup_plexdrive()
{
	local command_name="${FUNCNAME[0]}"

	local destination_rootpath="${HOME}"
	local destination_filename="${command_name}.$(date +\%Y\%m\%d\%H\%M\%S-\%b-\%d-\%a-\%H\%M).zip"
	local destination_path="${destination_rootpath}/${destination_filename}"
	local source_path="${HOME}/.plexdrive"

	if [[ -e "${source_path}" ]]; then
		echo -e "${command_name} start: $(date)"

		date > "/var/log/${command_name}.log" \
			&& zip -db -vr "${destination_path}" "${source_path}" \
			&& date >> "/var/log/${command_name}.log"

		echo -e "${command_name} end zip: $(date)"

		# Very specific to me. Temporary.
		if command_exists rclone; then
			rclone -v moveto ${destination_path} gd:servers/plex/${destination_filename} --stats 1s \
				&& echo -e "${command_name} end transfer: $(date)" \
				&& date >> "/var/log/${command_name}.log"
		fi
	fi
}

##
# Back up deluge files and state.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
backup_deluge()
{
	local command_name="${FUNCNAME[0]}"

	local destination_rootpath="${HOME}"
	local destination_filename="${command_name}.$(date +\%Y\%m\%d\%H\%M\%S-\%b-\%d-\%a-\%H\%M).zip"
	local destination_path="${destination_rootpath}/${destination_filename}"
	local source_path="/var/lib/deluge"

	if [[ -e "${source_path}" ]]; then
		echo -e "${command_name} start: $(date)"

		date > "/var/log/${command_name}.log" \
			&& zip -db -vr "${destination_path}" "${source_path}" \
			&& date >> "/var/log/${command_name}.log"

		echo -e "${command_name} end zip: $(date)"

		# Very specific to me. Temporary.
		if command_exists rclone; then
			rclone -v moveto ${destination_path} gd:servers/transfers/${destination_filename} --stats 1s \
				&& echo -e "${command_name} end transfer: $(date)" \
				&& date >> "/var/log/${command_name}.log"
		fi
	fi
}

# Temporary and specific.
plexbox_import_run()
{
	if command_exists rclone; then
		rclone -v copy gd:servers/plexbox ~/plexbox --stats 1s
		cd ~/plexbox && unzip "*.zip"

		if [[ ! -d ~/.plexdrive ]]; then
			mv ~/plexbox/root/.plexdrive ~
			if ! command_exists plexdrive; then
				~/.dotfiles/dpm.d/yum.d/plexdrive_install
			fi
		fi

		if [[ ! -d /var/lib/plexmediaserver ]]; then
			mv -f ~/plexbox/var/lib/plexmediaserver /var/lib
			rm -rf /var/lib/plexmediaserver/Library/Application\ Support/Plex\ Media\ Server/Preferences.xml
			rm -rf /var/lib/plexmediaserver/Library/Application\ Support/Plex\ Media\ Server/plexmediaserver.pid

			groupadd plex \
				; groupadd video \
				; adduser plex -g plex -G video -d /var/lib/plexmediaserver -s /sbin/nologin

			chown -R plex:plex /var/lib/plexmediaserver
		fi

		echo -e 'Run as non-root: bash -c "$(wget -qO - https://raw.githubusercontent.com/mrworf/plexupdate/master/extras/installer.sh)"'
		echo -e "Wait for it to install."
		echo -e "Run on your local machine: ssh $(ipp) -L 8888:localhost:32400"
		echo -e "Browse to: http://localhost:8888/web"
		echo -e "Add Dynamic DNS cron update for server to update domain with ephemereal IP."
	fi
}

##
# Get the inode number of a file.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
get_inode_number()
{
	if [[ -n $1 ]]; then
		local path="${1}"

		if [[ -e "${path}" ]]; then
			stat -c '%i' "${path}"
		fi
	fi
}

##
# Get the number of hardlinks that point to path.
#
# This is based on how many hardlinks share the same inode.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
get_hardlink_count()
{
	if [[ -n $1 ]]; then
		local path="${1}"

		if [[ -e "${path}" ]]; then
				stat -c '%h' "${path}"
		fi
	fi
}

##
# Get the path to a file based on the inode provided.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
get_paths_from_inode_number()
{
	if [[ -n $1 ]]; then
		local inode_number="${1}"
		#readlink -f $(find . -xdev -inum "${inode_number}")
		find . -xdev -inum "${inode_number}"
	fi
}

##
# Find all hardlinks and replace them with symlinks to plexdrive mount.
#
# Steps:
#
# - deluge downloads to /content/local/tmp # and SEEDS
# - sonarr hardlinks to /content/unionfs/media/tv/[show] # /content/local/media/tv/[show]
# - [There is now 1 inode and 2 links to it]
# - rclone moves to gd:media/tv/[show] # /content/plexdrive/media/tv/[show]
# - ensure uploads are complete
# - script recursively loops through seed directory /content/local/tmp and gets inode of each file
# - find every file path that matches inode numbers in /content/
# - find equivalent path in /content/plexdrive/media
# - force replace all hardlinks with symlinks to /content/plexdrive/media/tv/[show]
#
#
# File structure for this to work:
#
# /content
#   - /local (RW) [tmp items move here on complete]
#     - /media
#       - /movies
#       - /tv
#     - /tmp
#
#   - /plexdrive (RO)
#     - /media
#       - /movies
#       - /tv
#
#   - /unionfs (fuse mount) [local + plexdrive]
#     - /media
#       - /movies
#       - /tv
#     - /tmp
#
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
seed_hardlink_with_plexdrive_symlink()
{
	local basepath="~/content"
	if [[ -d "${basepath}" ]]; then
		echo -e "Searching for files from ${basepath}/local/tmp in ${basepath}/plexdrive"

		local tmpfiles=$(find "${basepath}/local/tmp" -type f)
		OIFS="$IFS"
		IFS=$'\n'
		for tmpfile in ${tmpfiles}; do
			local tmp_filename=$(basename "${tmpfile}")
			local tmp_filepath=$(readlink -s -q -f "${tmpfile}")
			if [[ -f "${tmp_filepath}" ]] \
				&& [[ ! -L "${tmp_filepath}" ]] \
			; then

				echo -e "\nSeeking: ${tmp_filepath}"
				local plexdrive_filepath=$(readlink -s -q -f "$(find ${basepath}/plexdrive/media -maxdepth 5 -type f -iname "${tmp_filename}")")
				if [[ -f "${plexdrive_filepath}" ]]; then
					#echo "${tmp_filepath}"
					#echo "${plexdrive_filepath}"

					echo -e "${tmp_filepath} will symlink to: ${plexdrive_filepath}"
					if [[ -n "${1}" ]]; then
						ln -nsfv "${plexdrive_filepath}" "${tmp_filepath}"
					fi
				fi
			fi
		done
		IFS="$OIFS"
	fi
}

create_content_structure()
{
	mkdir -v ~/content
	chmod 770 ~/content
	chmod g+s ~/content
	setfacl -R -m d:u::rwX,d:g::rwX,d:o::--X,u::rwX,g::rwX,o::--X ~/content

	mkdir -v ~/content/local
	mkdir -v ~/content/local/media
	mkdir -v ~/content/local/tmp
	mkdir -v ~/content/plexdrive
	mkdir -v ~/content/unionfs
}

timezone_set_est5edt()
{
	if [[ -e /usr/share/zoneinfo/EST5EDT ]]; then
		mv /etc/localtime /etc/localtime.bak \
			&& ln -nsfv /usr/share/zoneinfo/EST5EDT /etc/localtime
	fi
}


rclone_toggle_bwlimit()
{
	kill -SIGUSR2 $(pidof rclone)
}

##
# Import a MySQL dump piped through pv to get progress. Pimped.
#
# This will also gunzip a *.gz dumped file, if present. Errors will be ignored
# and printed to the terminal, so import can continue (--force).
#
# @author Dane MacMillan <work@danemacmillan.com>
# @link https://github.com/danemacmillan/dotfiles
# @license MIT
pimport()
{
	local filepath=""
	if [[ -n "$1" ]]; then
		local filepath="$1"
	fi

	local database_name=""
	if [[ -n "$2" ]]; then
		local database_name="$2"
	fi

	local username=${USER}
	if [[ -n "$3" ]]; then
		local username="$3"
	fi

	if [[ -z ${filepath} || -z ${database_name} ]]; then
		echo -e "Import a MySQL dump (*.sql|*.sql.gz) with progress."
		echo -e "Usage:\n  ${BCYAN}pimport${RESET} ${GREEN}FILEPATH DATABASE ${YELLOW}[USERNAME]${RESET}"
		return 1
	fi

	if [[ ! -f ${filepath} ]]; then
		echo -e "$filepath does not exist."
		return 1
	fi

	local now=$(date +'%Y%m%d%H%M%S-%b-%d-%a-%H%M')
	echo -e "${BBLUE}Import start: ${RESET}${WHITE}$now${RESET}"

	if [[ ${filepath} == *.gz ]]; then
		pv ${filepath} | gunzip | mysql -u${username} -p -D ${database_name} --force
	else
		pv ${filepath} | mysql -u${username} -p -D ${database_name} --force
	fi

	local now=$(date +'%Y%m%d%H%M%S-%b-%d-%a-%H%M')
	echo -e "${BBLUE}Import finish: ${RESET}${WHITE}$now${RESET}"
}

##
# Rsync copy
#
# This exists so that instead of using `scp` to copy files, you can just
# invoke `rcp` with predefined arguments that are good for transfers,
# including automatic resumption of partial uploads.
#
# Note that by default the progress of the entirety of the transfer is shown
# on one line using --info=progress2. Should you want to see the transfer
# progress of each file individually, simply make the command more verbose by
# passing the option `-v` or `-vv`, which overrides the consolidated progress.
# These options are simply passed directly to `rsync`. Read the `rsync`
# documentation to learn about available options.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @license MIT
rcp()
{
	local source=''
	if [[ -n "$1" ]]; then
		local source="$1"
	fi

	local destination=''
	if [[ -n "$2" ]]; then
		local destination="$2"
	fi

	local options=''
	if [[ -n "$3" ]]; then
		local options="$3"
	fi

	if [[ -n ${source} && -n ${destination} ]]; then
		rsync -hrltDz --info=progress2 --partial --partial-dir '$HOME/tmp/rsync-partials/' --exclude-from=$HOME/.rsync_excludes ${options} -e ssh ${source} ${destination}
	else
		echo -e "Rsync over SSH with useful defaults selected: partial file resumption, full progress."
		echo -e "\nUsage:\n  ${BCYAN}rcp${RESET} ${GREEN}[[USER@]HOST:]SOURCE [[USER@]HOST:]DESTINATION ${YELLOW}[-RSYNC_OPTIONS]${RESET}"
		echo -e "\nExamples:"
		echo -e "\n Rsync all contents of current directory to remote (defined in ~/.ssh/config as stage) directory foo in remote home directory:\n  rcp . stage:foo/ "
		echo -e "\n Same as above, but display verbose, per-file progress of transfer:\n  rcp . stage:foo/ -vv"
		echo -e "\n Rsync all contents of foo to remote bar directory, but display verbose, per-file progress of transfer:\n  rcp foo/ stage:bar -vv"
		return 1
	fi
}

##
# Migrate predefined local user private files to remote SSH user environment.
#
# That this will not remove the source files after migration. It will also
# create any necessary paths on the remote if they do not already exist.
#
# This is useful for migrating private SSH keys and other sensitive data to a
# new remote SSH user environment that typically should not exist in a public
# repository, like for dotfiles.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @license MIT
#
# @TODO Make it accept extra rsync and ssh parameters.
migrate_privates()
{
	local command_name="migrate_privates"

	local filepaths=( \
		".extra" \
		".gitconfig.local" \
		".ssh/id_rsa" \
		".ssh/id_rsa.pub" \
		".weechat/irc.conf" \
	)

	local destination=""
	if [[ -n "$1" ]]; then
		local destination="$1"
	fi

	if [[ -n ${destination} ]]; then
		for filepath in "${filepaths[@]}"; do
			if [[ -e "${HOME}/${filepath}" ]]; then
				local filepath_destination="${destination}"
				local filepath_base="$(dirname ${filepath})"

				# Handle separator to use based on destination pattern.
				local filepath_separator=""
				if [[ "${destination}" =~ ^.+\:.+$ ]]; then
					if [[ ! "${destination}" =~ ^.+\/$ ]]; then
						local filepath_separator="/"
					fi
				elif [[ "${destination}" =~ ^.+\:$ ]]; then
					local filepath_separator="~/"
				else
					local filepath_separator=":~/"
				fi

				local filepath_destination="${destination}${filepath_separator}${filepath_base}"

				# Figure out path to be created if it does not exist on the remote.
				[[ "${filepath_destination}" =~ ^(.+)\:(.*)$ ]]
				local host="${BASH_REMATCH[1]}"
				local build_path="${BASH_REMATCH[2]}"

				if [[ "${build_path}" =~ ^(.*)\/\.$ ]]; then
					local build_path="${BASH_REMATCH[1]}"
				fi

				if [[ "${build_path}" == "" ]]; then
					local build_path="~/"
				fi

				#echo -e "Separator: ${filepath_separator}"
				#echo -e "Path: ${filepath_destination}"
				#echo -e "Host: ${host}"
				#echo -e "Build: ${build_path}"

				rsync -hrLtDzpuvq --no-motd --rsync-path="mkdir -p ${build_path} && rsync" "${HOME}/${filepath}" "${filepath_destination}" -e "ssh -q" \
					&& echo -e "Migrated ${YELLOW}${HOME}/${filepath}${RESET} -> ${YELLOW}${destination}${filepath_separator}${filepath}${RESET}" \
					|| echo -e "${RED}Error migrating ${YELLOW}${filepath}${RESET} -> ${YELLOW}${filepath_destination}${RESET}"
			fi
		done
	else
		echo -e "Migrate predefined local user (${USER}) private files to remote SSH user environment."
		echo -e "\nAny destination path specified will be created if it does not exist. However, if the"
		echo -e "user does not have the permissions to create a directory at the path, it will fail."
		echo -e "\nDiscovered files that will migrate with this command:"
		for filepath in "${filepaths[@]}"; do
			if [[ -e "${HOME}/${filepath}" ]]; then
				echo -e "  - ${YELLOW}${HOME}/${filepath}${RESET}"
			fi
		done
		echo -e "\nUsage:\n  ${BCYAN}${command_name}${RESET} ${GREEN}USER@HOST[:PATH]${RESET}"
		echo -e "\nExamples:"
		echo -e "\n Migrate private files to a new remote host:\n  ${command_name} user@host"
		echo -e "\n Migrate private files to remote host (defined in ~/.ssh/config as stage):\n  ${command_name} stage"
		echo -e "\n Same as above, but to non-existent path on remote that will be created:\n  ${command_name} stage:~/foo/bar/nonexistent/path"
		return 1
	fi
}

##
# Deploy Dane MacMillan's dotfiles to a remote server.
#
# Note that this does not accept extra rsync or SSH parameters. Add a remote
# host to a `.ssh/config` file and reference the name, or ensure that the
# remote host is on the implied port of 22.
#
# THIS IS NOT WORKING. IT IS A WORK IN PROGRESS.
#
# @author Dane MacMillan <work@danemacmillan.com>
# @license MIT
function dotfiles_deploy_remote()
{
	local command_name="dotfiles_deploy_remote"

	local destination=""
	if [[ -n "$1" ]]; then
		local destination="$1"
	fi

	if [[ -n "${destination}" ]]; then
		local dotfiles_install_string="curl -sL https://raw.githubusercontent.com/danemacmillan/dotfiles/master/install | bash -"
		migrate_privates "${destination}"
#		ssh -t "${destination}" su ${USER} -c "${dotfiles_install_string}"
		migrate_privates "${destination}"
	else
		echo -e "Deploy Dane MacMillan's dotfiles to a remote server. This will ask for sudo password."
		echo -e "\nUsage:\n  ${BCYAN}${command_name}${RESET} ${GREEN}USER@HOST${RESET}"
		echo -e "\nExamples:"
		echo -e "\n Deploy dotfiles to remote server:\n  ${command_name} user@host"
		echo -e "\n Deploy dotfiles to remote server (defined in ~/.ssh/config as stage):\n  ${command_name} stage"
		return 1
	fi
}

##
# Set the rclone configuration password for current session.
#
# Documentation: http://rclone.org/docs/#configuration-encryption
#
# @author Dane MacMillan <work@danemacmillan.com>
# @license MIT
function set_rclone_password()
{
	read -s RCLONE_CONFIG_PASS
	export RCLONE_CONFIG_PASS
}

###############################################################################
# From other authors
###############################################################################

##
# Usage: extract <file>
# Description: extracts archived files / mounts disk images
# Note: .dmg/hdiutil is Mac OS X-specific.
# Credit: https://github.com/holman/dotfiles
extract ()
{
	if [ -f $1 ]; then
		case $1 in
			*.tar.bz2)  tar -jxvf $1 ;;
			*.tar.gz)   tar -zxvf $1 ;;
			*.bz2)      bunzip2 $1 ;;
			*.dmg)      hdiutil mount $1 ;;
			*.gz)       gunzip $1 ;;
			*.tar)      tar -xvf $1 ;;
			*.tbz2)     tar -jxvf $1 ;;
			*.tgz)      tar -zxvf $1 ;;
			*.zip)      unzip $1 ;;
			*.ZIP)      unzip $1 ;;
			*.pax)      cat $1 | pax -r ;;
			*.pax.Z)    uncompress $1 --stdout | pax -r ;;
			*.Z)        uncompress $1 ;;
			*)          echo "'$1' cannot be extracted/mounted via extract()" ;;
		esac
	else
		echo "'$1' is not a valid file"
	fi
}

##
# Determine size of a file or total size of a directory.
# Credit: https://github.com/mathiasbynens/dotfiles
fs()
{
	if du -b /dev/null > /dev/null 2>&1; then
		local arg=-sbh;
	else
		local arg=-sh;
	fi

	if [[ -n "$@" ]]; then
		du $arg -- "$@";
	else
		du $arg .[^.]* *;
	fi
}
